{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\my pc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import txtai\n",
    "import transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from txtai.embeddings import Embeddings\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = Embeddings({\n",
    "    \n",
    "    \"path\": \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 996\n"
     ]
    }
   ],
   "source": [
    "# Open the JSON file and load its content into a list (Change the path to the JSON file accordingly)\n",
    "with open(\"C:\\Test\\Dataset.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Now 'data' contains a list of dictionaries, each representing a record\n",
    "\n",
    "# To get the number of records in the JSON file, you can simply use len(data)\n",
    "number_of_records = len(data)\n",
    "print(f'Number of records: {number_of_records}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Title': 'FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Future Medical Imaging',\n",
       " 'Abstract': 'The recent advancements in artificial intelligence (AI) combined with the\\n extensive amount of data generated by todays clinical systems, has led to the\\n development of imaging AI solutions across the whole value chain of medical\\n imaging, including image reconstruction, medical image segmentation,\\n image-based diagnosis and treatment planning. Notwithstanding the successes and\\n future potential of AI in medical imaging, many stakeholders are concerned of\\n the potential risks and ethical implications of imaging AI solutions, which are\\n perceived as complex, opaque, and difficult to comprehend, utilise, and trust\\n in critical clinical applications. Despite these concerns and risks, there are\\n currently no concrete guidelines and best practices for guiding future AI\\n developments in medical imaging towards increased trust, safety and adoption.\\n To bridge this gap, this paper introduces a careful selection of guiding\\n principles drawn from the accumulated experiences, consensus, and best\\n practices from five large European projects on AI in Health Imaging. These\\n guiding principles are named FUTURE-AI and its building blocks consist of (i)\\n Fairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness\\n and (vi) Explainability. In a step-by-step approach, these guidelines are\\n further translated into a framework of concrete recommendations for specifying,\\n developing, evaluating, and deploying technically, clinically and ethically\\n trustworthy AI solutions into clinical practice.',\n",
       " 'Year': 2017,\n",
       " 'Record Type \\n(1 - Proposal, 2 - Thesis/Research, 3 - Project)': '2 (Thesis/Research)',\n",
       " 'Classification \\n(1 - Basic Research, 2 - Applied Research)\\\\': '1 (Basic Research)',\n",
       " 'PSCED': '50',\n",
       " 'Author': 'Sarah Miller'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSCED value for record at index 4: 50\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store the PSCED values\n",
    "psced_values = []\n",
    "\n",
    "# Iterate through a dataset of text records\n",
    "for record in data:\n",
    "    # Extract the \"PSCED\" field from each record and append it to the psced_values list\n",
    "    psced_value = record.get(\"PSCED\")\n",
    "    psced_values.append(psced_value)\n",
    "\n",
    "# Access the PSCED value at a specific index (e.g., index 4)\n",
    "psced_value_4 = psced_values[4]\n",
    "\n",
    "# Print the PSCED value at index 4\n",
    "print(f\"PSCED value for record at index 4: {psced_value_4}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.index(psced_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = embeddings.search(\"Study about Convolutional Neural Networks and it must be year 2015\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(714, 0.246088907122612)\n",
      "(689, 0.246088907122612)\n",
      "(575, 0.246088907122612)\n",
      "(548, 0.246088907122612)\n",
      "(527, 0.246088907122612)\n",
      "(468, 0.246088907122612)\n",
      "(442, 0.246088907122612)\n",
      "(329, 0.246088907122612)\n",
      "(292, 0.246088907122612)\n",
      "(25, 0.246088907122612)\n"
     ]
    }
   ],
   "source": [
    "for item in res:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "Title: Selective Segmentation Networks Using Top-Down Attention\n",
      "Abstract: Convolutional neural networks model the transformation of the input sensory\n",
      " data at the bottom of a network hierarchy to the semantic information at the\n",
      " top of the visual hierarchy. Feedforward processing is sufficient for some\n",
      " object recognition tasks. Top-Down selection is potentially required in\n",
      " addition to the Bottom-Up feedforward pass. It can, in part, address the\n",
      " shortcoming of the loss of location information imposed by the hierarchical\n",
      " feature pyramids. We propose a unified 2-pass framework for object segmentation\n",
      " that augments Bottom-Up \\convnets with a Top-Down selection network. We utilize\n",
      " the top-down selection gating activities to modulate the bottom-up hidden\n",
      " activities for segmentation predictions. We develop an end-to-end multi-task\n",
      " framework with loss terms satisfying task requirements at the two ends of the\n",
      " network. We evaluate the proposed network on benchmark datasets for semantic\n",
      " segmentation, and show that networks with the Top-Down selection capability\n",
      " outperform the baseline model. Additionally, we shed light on the superior\n",
      " aspects of the new segmentation paradigm and qualitatively and quantitatively\n",
      " support the efficiency of the novel framework over the baseline model that\n",
      " relies purely on parametric skip connections.\n",
      "Year: 2011\n",
      "Record Type \n",
      "(1 - Proposal, 2 - Thesis/Research, 3 - Project): 2\n",
      "Classification \n",
      "(1 - Basic Research, 2 - Applied Research)\\: 1\n",
      "PSCED: 18\n",
      "Author: Maria Gonzalez\n",
      "Similarity: 0.6368701457977295\n",
      "\n",
      "Text:\n",
      "Title: Truncated Hilbert Transform: Uniqueness and a Chebyshev series Expansion Approach\n",
      "Abstract: We derive a stronger uniqueness result if a function with compact support and\n",
      " its truncated Hilbert transform are known on the same interval by using the\n",
      " Sokhotski-Plemelj formulas. To find a function from its truncated Hilbert\n",
      " transform, we express them in the Chebyshev polynomial series and then suggest\n",
      " two methods to numerically estimate the coefficients. We present computer\n",
      " simulation results to show that the extrapolative procedure numerically works\n",
      " well.\n",
      "Year: 2018\n",
      "Record Type \n",
      "(1 - Proposal, 2 - Thesis/Research, 3 - Project): 1\n",
      "Classification \n",
      "(1 - Basic Research, 2 - Applied Research)\\: 1\n",
      "PSCED: 18\n",
      "Author: Andrej Novak\n",
      "Similarity: 0.6368701457977295\n",
      "\n",
      "Text:\n",
      "Title: Sufficient Representations for Categorical Variables\n",
      "Abstract: Many learning algorithms require categorical data to be transformed into real\n",
      " vectors before it can be used as input. Often, categorical variables are\n",
      " encoded as one-hot (or dummy) vectors. However, this mode of representation can\n",
      " be wasteful since it adds many low-signal regressors, especially when the\n",
      " number of unique categories is large. In this paper, we investigate simple\n",
      " alternative solutions for universally consistent estimators that rely on\n",
      " lower-dimensional real-valued representations of categorical variables that are\n",
      " \"sufficient\" in the sense that no predictive information is lost. We then\n",
      " compare preexisting and proposed methods on simulated and observational\n",
      " datasets.\n",
      "Year: 2021\n",
      "Record Type \n",
      "(1 - Proposal, 2 - Thesis/Research, 3 - Project): 2\n",
      "Classification \n",
      "(1 - Basic Research, 2 - Applied Research)\\: 1\n",
      "PSCED: 18\n",
      "Author: Ahmed Al-Mansoori\n",
      "Similarity: 0.6368701457977295\n",
      "\n",
      "Text:\n",
      "Title: A critical analysis of self-supervision, or what we can learn from a single image\n",
      "Abstract: We look critically at popular self-supervision techniques for learning deep\n",
      " convolutional neural networks without manual labels. We show that three\n",
      " different and representative methods, BiGAN, RotNet and DeepCluster, can learn\n",
      " the first few layers of a convolutional network from a single image as well as\n",
      " using millions of images and manual labels, provided that strong data\n",
      " augmentation is used. However, for deeper layers the gap with manual\n",
      " supervision cannot be closed even if millions of unlabelled images are used for\n",
      " training. We conclude that: (1) the weights of the early layers of deep\n",
      " networks contain limited information about the statistics of natural images,\n",
      " that (2) such low-level statistics can be learned through self-supervision just\n",
      " as well as through strong supervision, and that (3) the low-level statistics\n",
      " can be captured via synthetic transformations instead of using a large image\n",
      " dataset.\n",
      "Year: 2012\n",
      "Record Type \n",
      "(1 - Proposal, 2 - Thesis/Research, 3 - Project): 2\n",
      "Classification \n",
      "(1 - Basic Research, 2 - Applied Research)\\: 1\n",
      "PSCED: 18\n",
      "Author: Isabella Santos\n",
      "Similarity: 0.6368701457977295\n",
      "\n",
      "Text:\n",
      "Title: K-Net: Towards Unified Image Segmentation\n",
      "Abstract: Semantic, instance, and panoptic segmentations have been addressed using\n",
      " different and specialized frameworks despite their underlying connections. This\n",
      " paper presents a unified, simple, and effective framework for these essentially\n",
      " similar tasks. The framework, named K-Net, segments both instances and semantic\n",
      " categories consistently by a group of learnable kernels, where each kernel is\n",
      " responsible for generating a mask for either a potential instance or a stuff\n",
      " class. To remedy the difficulties of distinguishing various instances, we\n",
      " propose a kernel update strategy that enables each kernel dynamic and\n",
      " conditional on its meaningful group in the input image. K-Net can be trained in\n",
      " an end-to-end manner with bipartite matching, and its training and inference\n",
      " are naturally NMS-free and box-free. Without bells and whistles, K-Net\n",
      " surpasses all previous state-of-the-art single-model results of panoptic\n",
      " segmentation on MS COCO and semantic segmentation on ADE20K with 52.1% PQ and\n",
      " 54.3% mIoU, respectively. Its instance segmentation performance is also on par\n",
      " with Cascade Mask R-CNNon MS COCO with 60%-90% faster inference speeds. Code\n",
      " and models will be released at https://github.com/open-mmlab/mmdetection.\n",
      "Year: 2020\n",
      "Record Type \n",
      "(1 - Proposal, 2 - Thesis/Research, 3 - Project): 2\n",
      "Classification \n",
      "(1 - Basic Research, 2 - Applied Research)\\: 1\n",
      "PSCED: 18\n",
      "Author: Chen Wei\n",
      "Similarity: 0.6368701457977295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = embeddings.search(\"PSCED MUST BE 18\", 5)\n",
    "\n",
    "for r in res:\n",
    "    record = data[r[0]]\n",
    "    \n",
    "    # Print each field on a new line\n",
    "    text = \"\\n\".join(f\"{key}: {value}\" for key, value in record.items())\n",
    "    print(f\"Text:\\n{text}\")\n",
    "    print(f\"Similarity: {r[1]}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
